---
title: "Time Series"
author: "Gaurika_Tyagi"
date: "June 4, 2016"
output: html_document
---

##Reading Data

The scan() function assumes that the data for successive time
points is in a simple text file with one column. The kings dataset contains data on the age of death of successive kings of England. It ha 42 observations.

```{r}
kings_death <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
head(kings_death)

births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")

souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
```

##Converting data to time-series object

Time series analysis requires that the dataset is convrted to a time-series object before the analysis. By default ts() assumes that the data has been collected on an yearly basis. If the data has been collected on a monthly basis, we can set the frequency=12, for quarterly we set the frequency=4.

```{r}
death_timeseries <- ts(kings_death)
death_timeseries

births_timeseries <- ts(births, frequency=12, start=c(1946,1))
births_timeseries

#data is gathered monthly from January, 1987
souvenir_timeseries <- ts(souvenir, frequency=12, start=c(1987,1))
souvenir_timeseries
```

##Plotting time-series data

```{r}
par(mfrow = c(1,3))
plot.ts(death_timeseries)
plot.ts(births_timeseries)
plot.ts(souvenir_timeseries)
```

Time-series interpretation:
1. Kings' death data shows random fluctuations in the data that are roughly constant in size over time. Hence, this must be an additive model
2. Babies' birth times show seasonality, with peaks durng summers and troughs during winters
3. Souvenir data shows that the size of the seasonal fluctuations increase with time. Clearly his data is not an example of an additive model and will need to be transformed to obtain an additive model.

```{r}
log_souvenir_ts <- log(souvenir_timeseries)
plot.ts(log_souvenir_ts)
```

Now, the size of the seasonal fluctuations and random fluctuations in the log-transformed time series seem to be roughly constant over time, and do not depend on the level of the time series.

##Decomposing Time-Series Data

separating it into its constituent components

__Decomposing Non-Seasonal Data__

Decomposing Non-Seasonal time-series data comprizes of breaking down the non-seasonal data into a trend and an irregular component.

To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is
common to use a smoothing method, such as calculating the simple moving average of the time series.

The order/n of SMA is found out by exploratory analysis. The kings' death data appears non-seasonal and maybe additive. Let us start with analyzing that.

```{r}
#install.packages("TTR")
library(TTR)

kings_ts_SMA2 <- SMA(death_timeseries, n = 2)
plot.ts(kings_ts_SMA2)
#still there is a lot of random fluctuations in the time series smoothed using a simple moving average of order 2. Now, we try a higher order

kings_ts_SMA7 <- SMA(death_timeseries, n = 7)
plot.ts(kings_ts_SMA7)
```

The order 7 gives a clearer picture of the trend component, and we can see that the age of death of the kings decreased from over 50 years old to lesser than 40 years old during the reign of the first 20 kings, and then increased after that to about 73 years old by the end of the reign of the 40th king in the time series

__Decomposing Seasonal Data__

A seasonal time series consists of a trend component, a seasonal component and an irregular component. Decomposing such a data breaks it down into these three components.

The trend component that can be described using an additive model can be found out using the decompose(). It returns a list object with named elements, “seasonal”, “trend”, and “random”

The births dataset above showed seasonality of summer and winter. Let us see if it can be decomposed as an additive model of the three omponents as random fluctutations seemed constant over time.

```{r}
births_ts_components <- decompose(births_timeseries)
births_ts_components

#This shows the original values along with seasonal, random and trend components. It also shows that the seasonality appears to be additive.
```

Shows that the seasonal factor is the lowest for Feb and highest for July, indicating the highest births in July and lowest in February. 

This also shows that the seasonal factors remain more or less the same for every year (for respective months from /Jan-Dec).

```{r}
plot(births_ts_components)
```

The trend component shows a steady increase in birth rates since 1949.

Such an additiv seasonal model can be adjusted in time-series respect by estimating the seasonal component and subtracting it from the original data. This will give the main trend and the irregular component.

```{r}
births_ts_seasonally_adjusted <- births_timeseries - births_ts_components$seasonal
plot(births_ts_seasonally_adjusted)
```

##Forecasts (short-term) using Exponential Smoothing

__Simple Exponential Smoothing__

A time series model with a constant level and no seasonality can use simple exponential smoothing to describe additivity. 

<I>The simple exponential smoothing method provides a way of estimating the level at the current time point. Smoothing is controlled by the parameter alpha; for the estimate of the level at the current time point. The value of alpha; lies between 0 and 1. Values of alpha that are close to 0 mean that little weight is placed on the most recent observations when making forecasts of future values.</I>

Let us take the example of rainfall in inches from 1813-1912.

```{r}
#removing header
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)

rain_timeseries <- ts(rain, start = c(1813))
plot.ts(rain_timeseries)
```

HoltWinters() is used to achieve simple exponential smoothing by setting the parameters beta and gamma to FALSE. Byt default it takes the intial level as the first obbservation. We can set the level of the initial value by using the l.start parameter. Just like the decompose function, this also gives a list of variables.

Note: HoltWinters() gives the forecasts for the same time period as the data fed to it.

```{r}
rain_ts_forecasts <- HoltWinters(rain_timeseries, beta = FALSE, gamma = FALSE)
rain_ts_forecasts
#The aplha here gives how closely the forecasts are based on recent and not so recent observations.
```

Let us see the value of forecasts viz-a-viz the original data

```{r}
rain
rain_ts_forecasts$fitted
```

Plotting the forecasts: Original data is in black and forecasts are in red

```{r}
plot(rain_ts_forecasts)
```

Let us see the accuracy of these forecasts based on the sum of squared errors in the in-sample forecast errors, that is, the forecast errors for the time period covered by our original time series.

Errors are found as observed value - predicted value. These are stored in the list as "residuals"

```{r}
rain_ts_forecasts$SSE

#changing the initial level to start at a particular point
HoltWinters(rain_timeseries, beta = FALSE, gamma = FALSE, l.start = 24)
```

To make predictions beyond the time period in the data, we can use forecasts.HoltWinters(). This is within the forecast package. "h" parameter defines how many years do we want to predict. This does not give a fixed value of a forecast but an interval within which the values may falls

```{r}
#install.packages("forecast")
library(forecast)
rain_future <- forecast.HoltWinters(rain_ts_forecasts, h = 5)
```

Plotting the 5 year forecasts (original data is present till 1912)

```{r}
plot.forecast(rain_future)
```

Here the forecasts for 1913-1917 are plotted as a blue line, the 80% prediction interval as an darker blue shaded area,
and the 95% prediction interval as a lighter blue shaded area.

Errors
If there is a correlation between forecast error for successive predictions, the simple exponential forecasts can be improved upon by another forecasting technique. To find this, we obtain a correlogram of the in-sample forecast errors for lags 1-20. We can calculate a correlogram of the forecast errors using the “acf()” function in R. To specify the maximum lag that we want to look at, we use the “lag.max” parameter in acf().

```{r}
acf(rain_future$residuals, lag.max = 20)
```

You can see from the sample correlogram that the autocorrelation at lag 3 is just touching the significance bounds.
To test whether there is significant evidence for non-zero correlations at lags 1-20, we can carry out a Ljung-
Box test. This can be done in R using the “Box.test()”, function. The maximum lag that we want to look at is
specified using the “lag” parameter in the Box.test() function

```{r}
Box.test(rain_future$residuals, lag=20, type="Ljung-Box")
```

p-value of 0.6 and test statistic of 17.4 suggest non-zero auto-correlation in the in-sample forecast errors at lags 1-20.

We will also check if the forecast errors are normally districbuted with mean 0 and variance constant.

```{r}
plot.ts(rain_future$residuals)
#The plot shows that the in-sample forecast errors seem to have roughly constant variance over time, although the size of the fluctuations in the start of the time series (1820-1830) may be slightly less than that at later dates (eg.1840-1850)

#checking normality of errors
hist(rain_future$residuals, col = "green")
#Shows an almost normal distribution with mean 0
```

Box.test and histogram suggest that the simple exponential smoothing method provides an adequate predictive model.

__Holt's Exponential Smoothing__

