---
title: "Time Series"
author: "Gaurika_Tyagi"
date: "June 4, 2016"
output: html_document
---

##Reading Data

The scan() function assumes that the data for successive time
points is in a simple text file with one column. The kings dataset contains data on the age of death of successive kings of England. It ha 42 observations.

```{r}
kings_death <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
head(kings_death)

births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")

souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
```

##Converting data to time-series object

Time series analysis requires that the dataset is convrted to a time-series object before the analysis. By default ts() assumes that the data has been collected on an yearly basis. If the data has been collected on a monthly basis, we can set the frequency=12, for quarterly we set the frequency=4.

```{r}
death_timeseries <- ts(kings_death)
death_timeseries

births_timeseries <- ts(births, frequency=12, start=c(1946,1))
births_timeseries

#data is gathered monthly from January, 1987
souvenir_timeseries <- ts(souvenir, frequency=12, start=c(1987,1))
souvenir_timeseries
```

##Plotting time-series data

```{r}
par(mfrow = c(1,3))
plot.ts(death_timeseries)
plot.ts(births_timeseries)
plot.ts(souvenir_timeseries)
```

Time-series interpretation:
1. Kings' death data shows random fluctuations in the data that are roughly constant in size over time. Hence, this must be an additive model
2. Babies' birth times show seasonality, with peaks durng summers and troughs during winters
3. Souvenir data shows that the size of the seasonal fluctuations increase with time. Clearly his data is not an example of an additive model and will need to be transformed to obtain an additive model.

```{r}
log_souvenir_ts <- log(souvenir_timeseries)
plot.ts(log_souvenir_ts)
```

Now, the size of the seasonal fluctuations and random fluctuations in the log-transformed time series seem to be roughly constant over time, and do not depend on the level of the time series.

##Decomposing Time-Series Data

separating it into its constituent components

__Decomposing Non-Seasonal Data__

Decomposing Non-Seasonal time-series data comprizes of breaking down the non-seasonal data into a trend and an irregular component.

To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is
common to use a smoothing method, such as calculating the simple moving average of the time series.

The order/n of SMA is found out by exploratory analysis. The kings' death data appears non-seasonal and maybe additive. Let us start with analyzing that.

```{r}
#install.packages("TTR")
library(TTR)

kings_ts_SMA2 <- SMA(death_timeseries, n = 2)
plot.ts(kings_ts_SMA2)
#still there is a lot of random fluctuations in the time series smoothed using a simple moving average of order 2. Now, we try a higher order

kings_ts_SMA7 <- SMA(death_timeseries, n = 7)
plot.ts(kings_ts_SMA7)
```

The order 7 gives a clearer picture of the trend component, and we can see that the age of death of the kings decreased from over 50 years old to lesser than 40 years old during the reign of the first 20 kings, and then increased after that to about 73 years old by the end of the reign of the 40th king in the time series

__Decomposing Seasonal Data__

A seasonal time series consists of a trend component, a seasonal component and an irregular component. Decomposing such a data breaks it down into these three components.

The trend component that can be described using an additive model can be found out using the decompose(). It returns a list object with named elements, “seasonal”, “trend”, and “random”

The births dataset above showed seasonality of summer and winter. Let us see if it can be decomposed as an additive model of the three omponents as random fluctutations seemed constant over time.

```{r}
births_ts_components <- decompose(births_timeseries)
births_ts_components

#This shows the original values along with seasonal, random and trend components. It also shows that the seasonality appears to be additive.
```

Shows that the seasonal factor is the lowest for Feb and highest for July, indicating the highest births in July and lowest in February. 

This also shows that the seasonal factors remain more or less the same for every year (for respective months from /Jan-Dec).

```{r}
plot(births_ts_components)
```

The trend component shows a steady increase in birth rates since 1949.

Such an additiv seasonal model can be adjusted in time-series respect by estimating the seasonal component and subtracting it from the original data. This will give the main trend and the irregular component.

```{r}
births_ts_seasonally_adjusted <- births_timeseries - births_ts_components$seasonal
plot(births_ts_seasonally_adjusted)
```

##Forecasts (short-term) using Exponential Smoothing

__Simple Exponential Smoothing__

A time series model with a constant level and no seasonality can use simple exponential smoothing to describe additivity. 

<I>The simple exponential smoothing method provides a way of estimating the level at the current time point. Smoothing is controlled by the parameter alpha; for the estimate of the level at the current time point. The value of alpha; lies between 0 and 1. Values of alpha that are close to 0 mean that little weight is placed on the most recent observations when making forecasts of future values.</I>

Let us take the example of rainfall in inches from 1813-1912.

```{r}
#removing header
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)

rain_timeseries <- ts(rain, start = c(1813))
plot.ts(rain_timeseries)
```

HoltWinters() is used to achieve simple exponential smoothing by setting the parameters beta and gamma to FALSE. Byt default it takes the intial level as the first obbservation. We can set the level of the initial value by using the l.start parameter. Just like the decompose function, this also gives a list of variables.

Note: HoltWinters() gives the forecasts for the same time period as the data fed to it.

```{r}
rain_ts_forecasts <- HoltWinters(rain_timeseries, beta = FALSE, gamma = FALSE)
rain_ts_forecasts
#The aplha here gives how closely the forecasts are based on recent and not so recent observations.
```

Let us see the value of forecasts viz-a-viz the original data

```{r}
rain
rain_ts_forecasts$fitted
```

Plotting the forecasts: Original data is in black and forecasts are in red

```{r}
plot(rain_ts_forecasts)
```

Let us see the accuracy of these forecasts based on the sum of squared errors in the in-sample forecast errors, that is, the forecast errors for the time period covered by our original time series.

Errors are found as observed value - predicted value. These are stored in the list as "residuals"

```{r}
rain_ts_forecasts$SSE

#changing the initial level to start at a particular point
HoltWinters(rain_timeseries, beta = FALSE, gamma = FALSE, l.start = 24)
```

To make predictions beyond the time period in the data, we can use forecasts.HoltWinters(). This is within the forecast package. "h" parameter defines how many years do we want to predict. This does not give a fixed value of a forecast but an interval within which the values may falls

```{r}
#install.packages("forecast")
library(forecast)
rain_future <- forecast.HoltWinters(rain_ts_forecasts, h = 5)
```

Plotting the 5 year forecasts (original data is present till 1912)

```{r}
plot.forecast(rain_future)
```

Here the forecasts for 1913-1917 are plotted as a blue line, the 80% prediction interval as an darker blue shaded area,
and the 95% prediction interval as a lighter blue shaded area.

Errors
If there is a correlation between forecast error for successive predictions, the simple exponential forecasts can be improved upon by another forecasting technique. To find this, we obtain a correlogram of the in-sample forecast errors for lags 1-20. We can calculate a correlogram of the forecast errors using the “acf()” function in R. To specify the maximum lag that we want to look at, we use the “lag.max” parameter in acf().

```{r}
acf(rain_future$residuals, lag.max = 20)
```

You can see from the sample correlogram that the autocorrelation at lag 3 is just touching the significance bounds.
To test whether there is significant evidence for non-zero correlations at lags 1-20, we can carry out a Ljung-
Box test. This can be done in R using the “Box.test()”, function. The maximum lag that we want to look at is
specified using the “lag” parameter in the Box.test() function

```{r}
Box.test(rain_future$residuals, lag=20, type="Ljung-Box")
```

p-value of 0.6 and test statistic of 17.4 suggest non-zero auto-correlation in the in-sample forecast errors at lags 1-20.

We will also check if the forecast errors are normally districbuted with mean 0 and variance constant.

```{r}
plot.ts(rain_future$residuals)
#The plot shows that the in-sample forecast errors seem to have roughly constant variance over time, although the size of the fluctuations in the start of the time series (1820-1830) may be slightly less than that at later dates (eg.1840-1850)

#checking normality of errors
hist(rain_future$residuals, col = "green")
#Shows an almost normal distribution with mean 0
```

Box.test and histogram suggest that the simple exponential smoothing method provides an adequate predictive model.

__Holt's Exponential Smoothing__

A series with an increasing/decreasing trend and no seasonality, described using an additive model can use holt's exponential smoothing for short-term forecasts.

```{r}
skirt_diameters <- scan("http://robjhyndman.com/tsdldata/roberts/skirts.dat",skip=5)

skirt_ts <- ts(skirt_diameters, start=c(1866))
plot.ts(skirt_ts)
```

Skirt diameters increased till 1866 and then declined. We again use HoltWinters(), but only with gamma=FALSE

```{r}
skirt_forecast <- HoltWinters(skirt_ts, gamma = FALSE)
skirt_forecast
skirt_forecast$SSE
```

The estimated value of alpha is 0.84, and of beta is 1. These are both high, telling us that both the estimate
of the current value of the level, and of the slope b of the trend component, are based mostly upon very recent
observations in the time series. Makes sense because the slope changes considerably over time.

```{r}
plot(skirt_forecast)
#original in blak, forecast in red
```

pridicting for 19 more datapoints

```{r}
skirt_future <- forecast.HoltWinters(skirt_forecast, h = 19)
plot.forecast(skirt_future)
```

We check if predicted model could be improved in the same way.

__Holt-Winters Exponential Smoothing__

A time series that can be described using an additive model with increasing/decreasing trend along with <B>seasonality</B> can utilize Holt-Winters exponential smoothing to make short-term forecasts. 

This method estimates the level, slope and seasonal component at the current point. In this method, we utilize all alpha, beta and gamma for estimates of the level, slope b of the trend component, and the seasonal component respectively. All these have values between 0 nd 1. Values close to 0 emphasize on little weight on the most recent observation for future predictions.

This cn utilize the souvenir data:

```{r}
log_souvenir_ts_forecast <- HoltWinters(log_souvenir_ts)
log_souvenir_ts_forecast
```

The value of alpha (0.41)
is relatively low, indicating that the estimate of the level at the current time point is based upon both recent
observations and some observations in the more distant past. The value of beta is 0.00, indicating that the estimate
of the slope b of the trend component is not updated over the time series, and instead is set equal to its initial value.
This makes good intuitive sense, as the level changes quite a bit over the time series, but the slope b of the trend
component remains roughly the same. In contrast, the value of gamma (0.96) is high, indicating that the estimate
of the seasonal component at the current time point is just based upon very recent observations.

```{r}
plot(log_souvenir_ts_forecast)
```

This shows that there are seasonal peaks almost towards the end of every year.

Let us forecast the future now (10 more months)

```{r}
log_souvenir_ts_future <- forecast.HoltWinters(log_souvenir_ts_forecast, h = 10)
plot.forecast(log_souvenir_ts_future)
```

We will now investigate if the predicted model can be improved upon by checking whether in-sample forecast errors sho non-zero autocorrelation at lags 1-20, bu making correlogram and carrying out Ljung-Box test.

```{r}
#acf computes estimates of autocovariance or autocorrelation
acf(log_souvenir_ts_future$residuals, lag.max = 20)
Box.test(log_souvenir_ts_future$residuals, lag = 20, type = "Ljung-Box")
```

The correlation diagram above suggests that the in-sample forecast erros do not exceed the significance bounds for lags 1-20. p-value of 0.6 indicates little evidence of non-zero autocorrelations at lags 1-20.

Now, we check for constant variance, normal distribution, zero mean for forecast errors.

```{r}
plot.ts(log_souvenir_ts_future$residuals)
hist(log_souvenir_ts_future$residuals, col = "green")
```

Thus, Holt-Winters exponential smoothing provides an adequate predictive model of the log of sales at the souvenir shop, which probably cannot be improved upon

##ARIMA Models

Exponential smoothing methods are useful for making forecasts and make no assumptions about the correlations between successive values of time series. 

However, if you want to make predictions for forecasts made using exponential smoothing methods, the predition intervals require that the forecast made using exponential smoothing methods.

While exponential smoothing methods do not make any assumptions about correlations between successive values of the time series, in some cases you can make a better predictive model by taking correlations in the data into account. Autoregressive Integrated Moving Average (ARIMA) models include an explicit statistical model for the irregular component of a time series, that allows for non-zero autocorrelations in the irregular component.

__Differencing a Time Series__

Differencing is done to achieve stationary time series. This gives an ARIMA(p, d, q) model, where d is the order of differencing used.

The skirt data is not stationary as the mean changes a lot over time. Differencing it:

```{r}
skirt_diff <- diff(skirt_ts, differences = 1)
plot.ts(skirt_diff)

#This still does not give a stationary mean. Let us difference it twice
skirt_diff2 <- diff(skirt_ts, differences = 2)
plot.ts(skirt_diff2)
#appears stationary. Thus, order of differencing is 2.
```

Hence, we can make ARIMA(p,2,q) model for our time series. Now we need to find p and q

__Selecting a candidate ARIMA Model__

The p and q of the ARIMA model can be found out usingthe coellogram and partial corellogram (acf and pacf respectively). To get the actual values of the correlations we set plot=FALSE.

```{r}
acf(skirt_diff2, lag.max = 20)
acf(skirt_diff2, lag.max = 20, plot = FALSE)
```

We can see that at lag 5 the significance bound is exceeded, but all other lags between 1-20 do not exceed these bounds.

We now plot the partial correlogram for lags 1-20.

```{r}
pacf(skirt_diff2, lag.max = 20)
pacf(skirt_diff2, lag.max = 20, plot = FALSE)
```

partial autocorrelations at lags 5 and 10 exceed the significance bounds (negatively).

The correlograms, partial or actual fo not tail off to zero at any level. Hence, ARMA model is not possible for this series of difference 2.

Let us say the correlogramg for acf would have tailed off to 0 after lag1 and that for pacf would have tailed off to 0 at lag3. Then, ARMA models would have been possible for the time series data of difference2. After this we would have estimated the p and q for the model. Hence, the following models would have been possible:

1. ARMA(3,0) model, that is, an autoregressive model of order p=3, since the partial autocorrelogram is zero after lag3

2. ARMA(0,1) model, that is, a moving average model of order q=1, since the autocorrelogram is zero after lag 1 and the partial autocorrelogram tails off to zero

Principle of parsimony (model with the fewest parameters is the best) to decide the best model:
The ARMA(3,0) model has 3 parameters, the ARMA(0,1) model has 1 parameter, and the ARMA(p,q) model has at least 2 parameters. Therefore, the ARMA(0,1) model is taken as the best model.

This model is known as the ARMA model of order 1 or MA(1) model.

Example 2:

```{r}
volcano_dust <- scan ("http://robjhyndman.com/tsdldata/annual/dvi.dat", skip=1)
volcano_ts <- ts(volcano_dust, start = c(1500))
plot.ts(volcano_ts)
```

This shows that the random fluctuations are roughly constant over time, so an additive model is appropriate for describing this time series. Its mean and variance also appear to be stationary. Hence, we do not need differences to find the ARIMA model. 

Note: ARIMA(d, p, q) will have d=0

Now plotting acf and paf for lags 1-20 (to estimate p, q of the ARIMa model)

```{r}
acf(volcano_ts, lag.max = 20)
acf(volcano_ts, lag.max = 20, plot = FALSE)
```

Lags 1, 2 and 3 significantly exceed the significance bounds. The autocorrelations decresease in positive magnitude with increase in lag. 19 and 20 again exceed bounds, but this is probably due to chance (1 in 20 can exxceed bounds in 95% significance bounds).

```{r}
pacf(volcano_ts, lag.max = 20)
pacf(volcano_ts, lag.max = 20, plot = FALSE)
```

The positive partial autocorellation at lag 1 significantly exceeds the bounds. While the partial autocorrelation at lag 2 negatively exceeds significance level.

The correlogram tails off to zero after lag 3, and pacf after lag2. Hence, the following models are possible:
ARMA(2,0) since PACF trails off to 0 after lag 2 and ARMA(0,3) because autocorellogram is zero after lag 3 and partial correlogram is zero. Although perhaps too abruptly for this model to be appropriate). The third one which is possible is an ARMA(p,q) mixed model, since the correlogram and partial correlogram tail off to zero (although the partial correlogram perhaps tails off too abruptly for this model to be appropriate).

The ARMA(2,0) model has 2 parameters, the ARMA(0,3) model has 3 parameters, and the ARMA(p,q) model has at least 2 parameters. Therefore, using the principle of parsimony, the ARMA(2,0) model and ARMA(p,q) model are equally good candidate models.

ARMA model is ARMA(p=2, d= 0, q= 0)




```{r}
ARMA
```